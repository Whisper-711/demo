import os
import re
import json
import time
import math
import datetime
import logging
import pandas as pd
from pathlib import Path
from urllib import parse
from typing import Dict, List, Optional, Any
from drissionpage import WebPage, ChromiumOptions
import argparse


# 创建日志记录器
logger = logging.getLogger("BiorxivScraper")

def setup_logging(debug_mode=False):
    """设置日志级别和格式"""
    # 清除之前的处理程序
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # 设置日志级别
    if debug_mode:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
    
    # 创建控制台处理程序
    console_handler = logging.StreamHandler()
    
    # 创建文件处理程序
    file_handler = logging.FileHandler("biorxiv_scraper.log")
    
    # 设置日志格式
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)
    
    # 添加处理程序到记录器
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    # 设置过滤器，在非调试模式下过滤掉DEBUG级别的消息和论文处理成功的消息
    if not debug_mode:
        class CustomFilter(logging.Filter):
            def filter(self, record):
                # 只允许ERROR级别的消息和特定的INFO级别消息通过
                # 过滤掉包含"成功处理论文"、"进度更新"和"当前页面包含"的INFO消息
                if record.levelno == logging.INFO:
                    msg = record.getMessage()
                    if any(text in msg for text in ["成功处理论文", "进度更新", "当前页面包含", "处理页面"]):
                        return False
                return record.levelno == logging.ERROR or record.levelno == logging.INFO
        
        console_handler.addFilter(CustomFilter())

class Paper:
    def __init__(self):
        self.search_keyword = None
        self.title = None
        self.authors = None
        self.doi_date = None
        self.doi_ID = None
        self.detail_url = None
        self.posted_time = None
        self.posted_time_raw = None
        self.publish_text = None
        self.source_url = None
        self.run_id = None
        self.run_date = None
        self.insert_update_time = None

    def to_dict(self):
        """将论文对象转换为字典形式"""
        return {
            'search_keyword': self.search_keyword,
            'title': self.title,
            'authors': self.authors,
            'doi_date': self.doi_date,
            'doi_ID': self.doi_ID,
            'detail_url': self.detail_url,
            'posted_time': self.posted_time,
            'posted_time_raw': self.posted_time_raw,
            'publish_text': self.publish_text,
            'source_url': self.source_url,
            'run_id': self.run_id,
            'run_date': self.run_date,
            'insert_update_time': self.insert_update_time
        }

class BiorxivScraper:
    def __init__(self, output_dir: str = "data", proxy: str = "127.0.0.1:7890", debug_mode: bool = False, headless: bool = True):
        self.output_dir = output_dir
        self.run_date = datetime.datetime.now()
        self.papers = []
        self.proxy = proxy
        self.debug_mode = debug_mode
        self.headless = headless
        
        # 确保输出目录存在
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # 初始化WebPage
        self.init_browser()
    
    def init_browser(self):
        """初始化浏览器"""
        # 配置ChromiumOptions
        co = ChromiumOptions()
        
        # 设置代理
        if self.proxy:
            co.set_proxy(f"http://{self.proxy}")
        
        # 设置无头模式
        if self.headless:
            co.headless = True
        
        # 设置其他选项
        co.set_argument("--disable-blink-features=AutomationControlled")
        co.set_argument("--disable-infobars")
        co.set_argument("--disable-notifications")
        co.set_argument("--ignore-certificate-errors")
        co.set_argument("--no-sandbox")
        co.set_argument("--disable-gpu")
        
        # 创建WebPage
        self.page = WebPage(co=co)
        
        # 设置超时时间
        self.page.set.timeout(30)
        
        # 设置随机等待时间
        self.page.set.wait_after_click(0.5, 3)
        
        logger.info("浏览器初始化成功")
    
    def random_sleep(self, min_seconds=1, max_seconds=5):
        """随机等待，模拟人类行为"""
        import random
        sleep_time = min_seconds + random.random() * (max_seconds - min_seconds)
        time.sleep(sleep_time)
        return sleep_time
    
    def get_publish_text(self, doi_url: str) -> str:
        """获取论文的发布信息文本"""
        try:
            url = f'https://connect.biorxiv.org/bx_pub_doi_get.php?doi={doi_url}'
            
            # 使用请求页面方式获取JSON数据
            response = self.page.session.get(url, timeout=30)
            
            if not response.ok:
                logger.error(f"请求失败: {url}, 状态码: {response.status_code}")
                return 'None'
            
            try:
                json_data = response.json()
                if not json_data.get('pub') or len(json_data['pub']) == 0:
                    return 'None'

                pub_info = json_data['pub'][0]
                published_type = pub_info.get('pub_type', '')
                pub_doi = pub_info.get('pub_doi', '')
                pub_journal = pub_info.get('pub_journal', '')

                if not published_type:
                    return 'None'

                # 简化的文本生成逻辑
                if published_type:
                    return f"Published in {pub_journal} with DOI {pub_doi}"
                else:
                    return 'None'
            except Exception as e:
                logger.error(f"处理发布文本时出错: {str(e)}")
                return 'None'
        except Exception as e:
            logger.error(f"获取发布信息时出错: {str(e)}")
            return 'None'
    
    def get_posted_time(self, url_text: str) -> str:
        """获取论文发布时间"""
        try:
            url = f'https://www.biorxiv.org/content/{url_text}v1'
            
            # 使用SESSION模式，而不是浏览器模式来提高速度
            response = self.page.session.get(url, timeout=30)
            
            if not response.ok:
                logger.error(f"请求失败: {url}, 状态码: {response.status_code}")
                return ""
            
            # 使用正则表达式直接从HTML中提取发布日期
            date_pattern = r'Posted\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),\s+(\d{4})'
            date_match = re.search(date_pattern, response.text)
            
            if date_match:
                month = date_match.group(1)
                day = date_match.group(2)
                year = date_match.group(3)
                return f"{month} {day}, {year}"
            
            return ""
        except Exception as e:
            logger.error(f"获取发布时间时出错: {str(e)}")
            return ""

    def scrape_search_results(self, keywords: List[str], max_pages: int = None, max_papers: int = None) -> List[Paper]:
        """抓取搜索结果中的论文信息"""
        run_timestamp = self.run_date.strftime("%Y-%m-%d %H:%M:%S")
        total_papers_processed = 0
        papers_per_keyword = {}  # 记录每个关键词抓取的论文数量

        for keyword in keywords:
            logger.debug(f"开始抓取关键词 '{keyword}' 的数据")
            page_index = 0
            total_page = 1
            papers_per_keyword[keyword] = 0
            keyword_papers_count = 0

            while page_index < total_page:
                # 检查是否达到最大论文数量限制
                if max_papers and total_papers_processed >= max_papers:
                    logger.debug(f"已达到最大论文数量限制 ({max_papers})，停止抓取")
                    return self.papers
                
                # 构建搜索URL
                url = f'https://www.biorxiv.org/search/{parse.quote(keyword)}%20numresults%3A75%20sort%3Arelevance-rank?page={page_index}'
                logger.debug(f"处理页面 {page_index + 1}")
                
                try:
                    # 使用浏览器模式访问页面
                    self.page.get(url)
                    
                    # 等待页面加载完成
                    self.page.wait.load_complete()
                    
                    # 随机等待，模拟人类行为
                    self.random_sleep(2, 5)
                    
                    # 检查页面内容是否正确加载
                    if not self.page.ele_exist('.highwire-search-results-list'):
                        logger.error(f"无法找到搜索结果列表，跳过页面 {page_index + 1}")
                        # 保存页面以便调试
                        if self.debug_mode:
                            self.page.save.html(f"error_page_{page_index}.html")
                        page_index += 1
                        continue
                    
                    # 获取结果总数和总页数
                    summary_ele = self.page.ele('.highwire-search-summary')
                    if not summary_ele:
                        logger.error(f"无法找到搜索结果摘要，跳过页面 {page_index + 1}")
                        page_index += 1
                        continue
                    
                    summary_text = summary_ele.text
                    total_results_match = re.search(r'([\d,]+)\sResults', summary_text)
                    
                    if total_results_match:
                        total_results = int(total_results_match.group(1).replace(",", ""))
                        total_page = math.ceil(total_results / 75)
                        
                        if max_pages and max_pages < total_page:
                            total_page = max_pages
                        
                        logger.debug(f"找到 {total_results} 个结果，共 {total_page} 页")
                    
                    # 获取当前页的所有文章元素
                    article_elements = self.page.eles('.highwire-search-results-list > li')
                    logger.debug(f"当前页面包含 {len(article_elements)} 篇文章")
                    
                    for article_element in article_elements:
                        # 检查是否达到最大论文数量限制
                        if max_papers and total_papers_processed >= max_papers:
                            logger.debug(f"已达到最大论文数量限制 ({max_papers})，停止抓取")
                            return self.papers
                        
                        try:
                            paper = Paper()
                            
                            # 提取标题
                            title_ele = article_element.ele('.highwire-cite-linked-title > .highwire-cite-title')
                            if title_ele:
                                paper.title = title_ele.text
                            
                            # 提取日期和ID
                            date_id_ele = article_element.ele('.highwire-cite-metadata-pages')
                            if date_id_ele:
                                date_id = date_id_ele.text
                            
                            # 提取作者
                            authors_ele = article_element.ele('.highwire-cite-authors')
                            if authors_ele:
                                paper.authors = authors_ele.text
                            
                            # 提取DOI
                            doi_label_ele = article_element.ele('.doi_label')
                            if doi_label_ele and doi_label_ele.next:
                                doi_text = doi_label_ele.next.text
                                url_text = doi_text.replace('https://doi.org/', '').replace(' ', '')
                                
                                # 设置论文基本属性
                                paper.source_url = url
                                paper.detail_url = f'https://www.biorxiv.org/content/{url_text}v1'
                                paper.search_keyword = keyword
                                
                                # 处理日期和ID
                                date_id = date_id.split(";")[0]
                                if "." in date_id:
                                    paper.doi_ID = date_id.split(".")[-1]
                                    date_raw = date_id.rsplit('.', 1)[0]
                                    paper.doi_date = str(datetime.datetime.strptime(date_raw, '%Y.%m.%d').date())
                                else:
                                    paper.doi_ID = date_id
                                    paper.doi_date = None
                                
                                # 获取发布时间（使用SESSION模式）
                                posted_time_raw = self.get_posted_time(url_text)
                                if posted_time_raw:
                                    paper.posted_time_raw = posted_time_raw
                                    time_raw = posted_time_raw.strip('.')
                                    try:
                                        # 尝试解析日期
                                        parsed_date = datetime.datetime.strptime(time_raw, '%B %d, %Y')
                                        paper.posted_time = datetime.datetime.strftime(parsed_date, '%Y-%m-%d %H:%M:%S')
                                    except Exception as e:
                                        logger.warning(f"日期解析错误: {time_raw}, 错误: {str(e)}")
                                        paper.posted_time = None
                                else:
                                    paper.posted_time_raw = None
                                    paper.posted_time = None
                                
                                # 获取发布文本
                                paper.publish_text = self.get_publish_text(url_text)
                                
                                # 设置运行信息
                                paper.run_id = run_timestamp
                                paper.run_date = run_timestamp
                                paper.insert_update_time = run_timestamp
                                
                                # 添加论文到列表
                                self.papers.append(paper)
                                total_papers_processed += 1
                                keyword_papers_count += 1
                                papers_per_keyword[keyword] = keyword_papers_count
                                
                                # 在调试模式下打印每篇论文的处理信息
                                logger.debug(
                                    f"成功处理论文: {paper.title} ({total_papers_processed}/{max_papers if max_papers else '无限制'}) [关键词'{keyword}': {keyword_papers_count}篇]"
                                )
                                
                                # 随机等待，避免请求过快
                                self.random_sleep(0.5, 2)
                                
                        except Exception as e:
                            logger.error(f"处理文章时出错: {str(e)}")
                    
                    # 翻页前等待随机时间
                    self.random_sleep(3, 6)
                    page_index += 1
                    
                except Exception as e:
                    logger.error(f"处理搜索页面时出错: {str(e)}")
                    # 等待较长时间后重试
                    self.random_sleep(5, 10)
                    # 如果连续失败，考虑重新初始化浏览器
                    self.init_browser()
                    continue
            
            # 关键词之间等待更长时间
            self.random_sleep(5, 10)

        # 显示每个关键词抓取的论文数量
        for keyword, count in papers_per_keyword.items():
            logger.debug(f"关键词 '{keyword}' 共抓取了 {count} 篇论文")
        
        return self.papers

    def save_to_csv(self, filename_template: str = "biorxiv_{}.csv") -> str:
        """将抓取的论文保存到CSV文件"""
        if not self.papers:
            logger.warning("没有论文可保存")
            return None

        filename = filename_template.format(self.run_date.strftime("%Y-%m-%d"))
        filepath = os.path.join(self.output_dir, filename)

        # 转换论文列表为DataFrame
        papers_dict = [paper.to_dict() for paper in self.papers]
        df = pd.DataFrame(papers_dict)

        # 保存到CSV
        df.to_csv(filepath, index=False)
        logger.info(f"数据已保存到 {filepath}, 共 {len(self.papers)} 篇论文")

        return filepath
    
    def save_to_json(self, filename_template: str = "biorxiv_{}.json") -> str:
        """将抓取的论文保存到JSON文件"""
        if not self.papers:
            logger.warning("没有论文可保存")
            return None

        filename = filename_template.format(self.run_date.strftime("%Y-%m-%d"))
        filepath = os.path.join(self.output_dir, filename)

        # 转换论文列表为字典列表
        papers_dict = [paper.to_dict() for paper in self.papers]
        
        # 保存到JSON
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(papers_dict, f, ensure_ascii=False, indent=2)
            
        logger.info(f"数据已保存到 {filepath}, 共 {len(self.papers)} 篇论文")

        return filepath
    
    def close(self):
        """关闭浏览器"""
        self.page.quit()
        logger.info("浏览器已关闭")

def parse_arguments():
    """解析命令行参数"""
    parser = argparse.ArgumentParser(description='BioRxiv论文爬虫 (DrissionPage版)')
    parser.add_argument('--debug', action='store_true', help='启用调试模式')
    parser.add_argument('--no-headless', action='store_true', help='关闭无头模式，显示浏览器')
    parser.add_argument('--max-papers', type=int, default=None, help='最大论文数量限制')
    parser.add_argument('--max-pages', type=int, default=None, help='最大页数限制')
    parser.add_argument('--output-dir', type=str, default="data", help='输出目录')
    parser.add_argument('--keywords', type=str, nargs='+', default=['visium', '"10x" chromium'], help='搜索关键词列表')
    
    return parser.parse_args()

def main():
    """主函数"""
    # 解析命令行参数
    args = parse_arguments()
    
    # 设置日志级别
    setup_logging(debug_mode=args.debug)
    
    # 创建爬虫实例，设置固定代理
    proxy = "127.0.0.1:7890"  # 固定代理地址
    headless = not args.no_headless  # 默认启用无头模式
    
    try:
        scraper = BiorxivScraper(
            output_dir=args.output_dir,
            proxy=proxy,
            debug_mode=args.debug,
            headless=headless
        )
        
        # 设置搜索关键词
        keywords = args.keywords
        
        # 设置测试模式参数
        max_papers = args.max_papers  # 设置为None可以抓取所有论文
        max_pages = args.max_pages  # 设置为None可以抓取所有页面
        
        # 抓取数据
        logger.info(f"开始抓取bioRxiv数据，使用代理: {proxy}")
        logger.info(f"运行模式: {'调试模式' if args.debug else '正常模式'}, {'有头模式' if not headless else '无头模式'}")
        logger.info(f"搜索关键词: {keywords}")
        logger.info(
            f"限制: 最多抓取 {max_papers if max_papers else '无限制'} 篇论文，最多 {max_pages if max_pages else '无限制'} 页"
        )
        
        # 记录开始时间
        start_time = time.time()
        
        # 执行爬取
        papers = scraper.scrape_search_results(keywords, max_pages=max_pages, max_papers=max_papers)
        
        # 记录结束时间并计算耗时
        end_time = time.time()
        elapsed_time = end_time - start_time
        hours, remainder = divmod(elapsed_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        
        if papers:
            # 保存为CSV
            csv_path = scraper.save_to_csv()
            
            # 保存为JSON
            json_path = scraper.save_to_json()
            
            logger.info(
                f"爬取完成，共获取 {len(papers)} 篇论文，耗时: {int(hours)}小时{int(minutes)}分钟{int(seconds)}秒"
            )
            logger.info(f"数据已保存到CSV: {csv_path}")
            logger.info(f"数据已保存到JSON: {json_path}")
        else:
            logger.warning("未找到任何论文")
    
    except Exception as e:
        logger.error(f"程序运行出错: {str(e)}")
    
    finally:
        # 确保关闭浏览器
        if 'scraper' in locals():
            scraper.close()


if __name__ == "__main__":
    main()
